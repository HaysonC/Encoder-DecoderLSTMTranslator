{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5e165ae15d9f65",
   "metadata": {
    "id": "5f5e165ae15d9f65"
   },
   "source": [
    "# Sequence to Sequence Learning with Keras\n",
    "Author: Hayson Cheung [hayson.cheung@mail.utoronto.ca]\n",
    "\n",
    "In this notebook, we learn from the works of Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Sequence to Sequence Learning with Neural Networks, NIPS 2014. We will implement a simple sequence to sequence model using LSTM in Keras. The model will be trained on a dataset of English sentences and their corresponding German sentences. The model will be able to translate English sentences to German sentences.\n",
    "\n",
    "We map sequences of English words to sequences of German words. The model is trained on a dataset of English sentences and their corresponding German sentences. The model will be able to translate English sentences to German sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d1a0d6d2f9cfb",
   "metadata": {
    "id": "2e3d1a0d6d2f9cfb"
   },
   "source": []
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2025-01-27T22:17:13.461116Z",
     "start_time": "2025-01-27T22:17:13.231894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample.ipynb\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# sample.ipynb\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input, LSTM, Dense\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adam\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1FAO_IG3jlBd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FAO_IG3jlBd",
    "outputId": "633808f3-06b9-4ae2-ca83-54a6c29b5ac4",
    "ExecuteTime": {
     "end_time": "2025-01-27T22:19:18.141736Z",
     "start_time": "2025-01-27T22:19:18.125078Z"
    }
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "# for GPU \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m# for GPU \u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNum GPUs Available: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(tf\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mlist_physical_devices(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "fec8c0e7e94763e4",
   "metadata": {
    "id": "fec8c0e7e94763e4"
   },
   "source": [
    "# Parameters\n",
    "\n",
    "# Latent dimension is the number of hidden units |h(t)| in the LSTM cell\n",
    "LATENT_DIM = 256"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc6b4ce2521bad5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "bc6b4ce2521bad5f",
    "outputId": "22cc28f0-c74e-411d-de7d-96d8d259b581"
   },
   "source": [
    "import load_data\n",
    "\n",
    "load_data.main(\"de-en.tmx\")\n",
    "\n",
    "from load_data import INPUT_VOCAB_SIZE, OUTPUT_VOCAB_SIZE, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH, input_tokenizer, \\\n",
    "    output_tokenizer\n",
    "\n",
    "print(f\"Input vocab size: {INPUT_VOCAB_SIZE}\")\n",
    "print(f\"Output vocab size: {OUTPUT_VOCAB_SIZE}\")\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"Max output length: {MAX_OUTPUT_LENGTH}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10014d442ffb5d01",
   "metadata": {
    "id": "10014d442ffb5d01"
   },
   "source": [
    "## ENCODER and DECODER\n",
    "\n",
    "In the two LSTM models, the encoder LSTM model will take the input sequence and return the encoder states. The decoder LSTM model will take the output sequence and the encoder states as input and return the output sequence. The encoder and decoder models are defined separately and then combined to form the final model."
   ]
  },
  {
   "cell_type": "code",
   "id": "adcddf97abde4754",
   "metadata": {
    "id": "adcddf97abde4754"
   },
   "source": [
    "# Define Encoder\n",
    "encoder_input = Input(shape=(MAX_INPUT_LENGTH,))\n",
    "\n",
    "encoder_embedding = Embedding(INPUT_VOCAB_SIZE, LATENT_DIM)(encoder_input)\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define Decoder\n",
    "decoder_input = Input(shape=(MAX_OUTPUT_LENGTH,))\n",
    "decoder_embedding = Embedding(OUTPUT_VOCAB_SIZE, LATENT_DIM)(decoder_input)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Softmax means output is a probability distribution, and enhances the maximum probability output\n",
    "# dense layer is a regular densely-connected NN layer with softmax activation\n",
    "decoder_dense = Dense(OUTPUT_VOCAB_SIZE, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_outputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5680cca9a102c03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "b5680cca9a102c03",
    "outputId": "863e018c-c411-4fff-aade-dd1a2c652442"
   },
   "source": [
    "# Define the model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "beb6acb2f8991bb8",
   "metadata": {
    "id": "beb6acb2f8991bb8"
   },
   "source": [
    "## Training the Model\n",
    "This is where we train the model. We use the encoder input and decoder input to predict the decoder output. The model is trained on the dataset of English sentences and their corresponding German sentences.\n",
    "\n",
    "This takes a while to run. We can save the model and load it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b2636dcad0c08",
   "metadata": {
    "id": "499b2636dcad0c08"
   },
   "source": [
    "### Explaination of the data set\n",
    "encoder_input_train: Training data for the encoder (German sentences).\n",
    "decoder_input_train: Training data for the decoder (English sentences with <start> token).\n",
    "decoder_target_train: Target data for the decoder (English sentences).\n",
    "\n",
    "encoder_input_val: Validation data for the encoder (German sentences).\n",
    "decoder_input_val: Validation data for the decoder (English sentences with <start> token).\n",
    "decoder_target_val: Target data for the decoder (English sentences).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f75499163de2427",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f75499163de2427",
    "outputId": "48083416-41aa-4da4-fecd-e3065172fa5a"
   },
   "source": [
    "# Data Set Preparation\n",
    "from load_data import encoder_input_train, decoder_input_train, decoder_target_train, encoder_input_val, decoder_input_val, decoder_target_val\n",
    "with tf.device('/GPU:0'):\n",
    "  model.fit(\n",
    "      [encoder_input_train, decoder_input_train],  # Inputs for encoder and decoder\n",
    "      decoder_target_train,  # Target data for decoder\n",
    "      batch_size=16,  # Adjust as needed\n",
    "      epochs=30,  # Adjust as needed\n",
    "      validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "      verbose=1\n",
    "  )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "p6lUdbaJlIZ1",
   "metadata": {
    "id": "p6lUdbaJlIZ1"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd31bba1f272193",
   "metadata": {
    "id": "6fd31bba1f272193"
   },
   "source": [
    "As you can see, it would take forever to train the model. If we use the ted dataset, lets use soemthing simpler."
   ]
  },
  {
   "cell_type": "code",
   "id": "5a3b6fd34ed1a2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a3b6fd34ed1a2b",
    "outputId": "0cd0c059-3ace-4886-8566-d81b7d642e16"
   },
   "source": [
    "model.save(\"/content/seq2seq_model.h5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "from load_data import INPUT_VOCAB_SIZE, OUTPUT_VOCAB_SIZE, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH, input_tokenizer, output_tokenizer\n",
    "model = load_model(\"seq2seq_model.h5\")\n",
    "\n",
    "# set up the encoder and decoder, from the trained model\n",
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding = Embedding(OUTPUT_VOCAB_SIZE, LATENT_DIM)(decoder_input)\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input] + decoder_states_inputs,  # input: [decoder_input, h, c]\n",
    "    [decoder_outputs] + decoder_states  # output: [output, h, c]\n",
    ")\n",
    "\n",
    "# map indexes back into real words\n",
    "idx2word_input = {v:k for k, v in input_tokenizer.word_index.items()}\n",
    "idx2word_target = {v:k for k, v in output_tokenizer.word_index.items()}\n",
    "import numpy as np\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Step 1: Get encoder states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Step 2: Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index['sos']\n",
    "\n",
    "    # Step 3: Loop to generate the translated sequence\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = idx2word_target.get(sampled_token_index, '<UNK>')\n",
    "\n",
    "        # Append the sampled word to the decoded sentence\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n",
    "        if (sampled_word == 'eos' or len(decoded_sentence.split()) > MAX_OUTPUT_LENGTH):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "def translate(input_text):\n",
    "    # Tokenize the input sequence\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Get the translated sentence\n",
    "    translated_sentence = decode_sequence(input_seq)\n",
    "    return translated_sentence\n"
   ],
   "id": "64c17ce316e5adfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "print(translate(\"Ich bin ein Student.\"))  # I am a student.\n",
    "print(translate(\"Ich bin traurig.\"))  # I am sad.\n",
    "print(translate(\"Ich bin mude.\"))  # I am tired.\n",
    "        \n",
    "\n"
   ],
   "id": "caadc31af994afe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b88ddc7de326ab2c"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
