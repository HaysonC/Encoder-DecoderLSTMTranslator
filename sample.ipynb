{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5e165ae15d9f65",
   "metadata": {
    "id": "5f5e165ae15d9f65"
   },
   "source": [
    "# Sequence to Sequence Learning with Keras (Beta)\n",
    "Author: Hayson Cheung [hayson.cheung@mail.utoronto.ca]\\\n",
    "Adapted from: Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n",
    "\n",
    "In this notebook, we learn from the works of Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Sequence to Sequence Learning with Neural Networks, NIPS 2014. We will implement a simple sequence to sequence model using LSTM in Keras. The model will be trained on a dataset of English sentences and their corresponding German sentences. The model will be able to translate English sentences from German sentences.\n",
    "\n",
    "We map sequences of English words from sequences of German words. The model is trained on a dataset of English sentences and their corresponding German sentences. The goal of the model is to be able to translate English sentences from German sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txeNiMPTPODp",
   "metadata": {
    "id": "txeNiMPTPODp"
   },
   "source": [
    "## Initialization & Hyper Params\n",
    "\n",
    "Import tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# sample.ipynb\n",
    "print(\"Importing Tensorflow\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Embedding\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "Ne6SfdmCPR0u",
   "metadata": {
    "id": "Ne6SfdmCPR0u"
   },
   "source": [
    "IT WOULD TAKE INSANELY LONG IF U TRAIN IT URSELF WITH A CPU"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if GPU is available\n",
    "import tensorflow as tf\n",
    "CPU_FALLBACK = False\n",
    "num_GPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "if num_GPUs > 0:\n",
    "    print(f\"Number of GPUs: {num_GPUs}\")\n",
    "else:\n",
    "    print(\"No GPUs available, the code is intended to be run on a GPU for faster training.\")\n",
    "    import inquirer\n",
    "    questions = [\n",
    "        inquirer.List('continue',\n",
    "                      message=\"Do you want to continue training on a CPU?\",\n",
    "                      choices=['Yes', 'No'],\n",
    "                  ),\n",
    "    ]\n",
    "    answers = inquirer.prompt(questions)\n",
    "    if answers['continue'] != 'Yes':\n",
    "        print(\"Exiting...\")\n",
    "        exit()\n",
    "        \n",
    "if not tf.test.is_gpu_available():\n",
    "    CPU_FALLBACK = True\n",
    "    print(\"Tensorflow is running on CPU\")\n",
    "        "
   ],
   "id": "f222a5d0afd5b58d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "kazOxjY6PX2x",
   "metadata": {
    "id": "kazOxjY6PX2x"
   },
   "source": [
    "Define the dimension of the latent space. It is a hyperparameter. Typically, we take powers of 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "fec8c0e7e94763e4",
   "metadata": {
    "id": "fec8c0e7e94763e4"
   },
   "source": [
    "# Parameters\n",
    "\n",
    "# Latent dimension is the number of hidden units |h(t)| in the LSTM cell\n",
    "\n",
    "# IN ACCORDANCE TO THE PAPER:\n",
    "\"\"\" \n",
    "LATENT_DIM = 1024 # Number of LSTM units per layer\n",
    "EMBEDDING_DIM = 1024  # Embedding dimension\n",
    "NUM_LAYERS = 4  # Deep LSTM layers\n",
    "\"\"\"\n",
    "\n",
    "# SMALLER DIMENSIONS FOR TESTING\n",
    "LATENT_DIM = 256 \n",
    "EMBEDDING_DIM = 256\n",
    "NUM_LAYERS = 4\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sRh6H_x17B4p",
   "metadata": {
    "id": "sRh6H_x17B4p"
   },
   "source": [
    "## Load Data (Make sure of the path to the data file)\n",
    "\n",
    "Choose a data set online in tmx format, it shall start with <tu> then <seq>English<\\seq>\n",
    "<seq>Deutsch<\\seq>\n",
    "then\n",
    "<\\tu>"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc6b4ce2521bad5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc6b4ce2521bad5f",
    "outputId": "c7a2a650-ab02-44d7-9fba-688c939b073e"
   },
   "source": [
    "import load_data\n",
    "\n",
    "load_data.main()\n",
    "\n",
    "from load_data import INPUT_VOCAB_SIZE, OUTPUT_VOCAB_SIZE, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH\n",
    "\n",
    "print(f\"Input vocab size: {INPUT_VOCAB_SIZE}\")\n",
    "print(f\"Output vocab size: {OUTPUT_VOCAB_SIZE}\")\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"Max output length: {MAX_OUTPUT_LENGTH}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10014d442ffb5d01",
   "metadata": {
    "id": "10014d442ffb5d01"
   },
   "source": [
    "## ENCODER and DECODER\n",
    "\n",
    "In the two LSTM models, the encoder LSTM model will take the input sequence and return the encoder states. The decoder LSTM model will take the output sequence and the encoder states as input and return the output sequence. The encoder and decoder models are defined separately and then combined to form the final model.\n",
    "\n",
    "We also would like to implment a learning rate schedule, the paper rescricts lr after 5 epochs"
   ]
  },
  {
   "metadata": {
    "id": "adcddf97abde4754"
   },
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define Encoder\n",
    "encoder_input = Input(shape=(MAX_INPUT_LENGTH,))\n",
    "encoder_embedding = Embedding(INPUT_VOCAB_SIZE, EMBEDDING_DIM)(encoder_input)\n",
    "\n",
    "encoder_lstm = []\n",
    "encoder_states = []\n",
    "x = encoder_embedding\n",
    "for i in range(NUM_LAYERS):\n",
    "    return_state = (i == NUM_LAYERS - 1)  # Only return state for the last layer\n",
    "    lstm_layer = LSTM(LATENT_DIM, return_sequences=True, return_state=True, kernel_initializer=tf.keras.initializers.RandomUniform(-0.08, 0.08))\n",
    "    if return_state:\n",
    "        x, state_h, state_c = lstm_layer(x)\n",
    "        encoder_states = [state_h, state_c]\n",
    "    else:\n",
    "        x = lstm_layer(x)[0]\n",
    "    encoder_lstm.append(lstm_layer)\n",
    "\n",
    "# Define Decoder\n",
    "decoder_input = Input(shape=(MAX_OUTPUT_LENGTH,))\n",
    "decoder_embedding = Embedding(OUTPUT_VOCAB_SIZE, EMBEDDING_DIM)(decoder_input)\n",
    "\n",
    "decoder_lstm = []\n",
    "x = decoder_embedding\n",
    "for i in range(NUM_LAYERS):\n",
    "    return_state = i == NUM_LAYERS - 1 \n",
    "    lstm_layer = LSTM(LATENT_DIM, return_sequences=True, return_state=True, kernel_initializer=tf.keras.initializers.RandomUniform(-0.08, 0.08))\n",
    "    if return_state:\n",
    "        x, _, _ = lstm_layer(x, initial_state=encoder_states)\n",
    "    else:\n",
    "        x = lstm_layer(x)[0]\n",
    "    decoder_lstm.append(lstm_layer)\n",
    "\n",
    "# Output Layer\n",
    "decoder_dense = Dense(OUTPUT_VOCAB_SIZE, activation='softmax')\n",
    "decoder_output = decoder_dense(x)"
   ],
   "id": "adcddf97abde4754",
   "outputs": []
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "b5680cca9a102c03",
    "outputId": "6f378cf5-85e8-4696-9c76-d9b416513495"
   },
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define Model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "# Optimizer with Gradient Clipping\n",
    "optimizer = SGD(learning_rate=0.7, clipnorm=5)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "id": "b5680cca9a102c03",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "beb6acb2f8991bb8",
   "metadata": {
    "id": "beb6acb2f8991bb8"
   },
   "source": [
    "## Training the Model\n",
    "This is where we train the model. We use the encoder input and decoder input to predict the decoder output. The model is trained on the dataset of English sentences and their corresponding German sentences.\n",
    "\n",
    "This takes a while to run. We can save the model and load it later. Below is the lr (learning rate) schedule:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch >= 5:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)"
   ],
   "id": "f58391004a79c2a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "499b2636dcad0c08",
   "metadata": {
    "id": "499b2636dcad0c08"
   },
   "source": [
    "### Explaination of the data set\n",
    "encoder_input_train: Training data for the encoder (German sentences).\n",
    "decoder_input_train: Training data for the decoder (English sentences with <start> token).\n",
    "decoder_target_train: Target data for the decoder (English sentences).\n",
    "\n",
    "encoder_input_val: Validation data for the encoder (German sentences).\n",
    "decoder_input_val: Validation data for the decoder (English sentences with <start> token).\n",
    "decoder_target_val: Target data for the decoder (English sentences).\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data Set Preparation\n",
    "from load_data import encoder_input_train, decoder_input_train, decoder_target_train, encoder_input_val, decoder_input_val, decoder_target_val\n",
    "with tf.device('/GPU:0') if not CPU_FALLBACK else tf.device('/CPU:0'):\n",
    "    history = model.fit(\n",
    "        [encoder_input_train, decoder_input_train],\n",
    "        decoder_target_train,\n",
    "        batch_size=128,\n",
    "        epochs=8,\n",
    "        validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "        callbacks=[lr_callback],\n",
    "        verbose=1\n",
    "    )"
   ],
   "id": "f9cf1cee6281f3cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plotting the Training Loss",
   "id": "a937658567584ffd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "40c2551590cf8c42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, it would take forever to train the model (little more than 2h).\n",
    "This is actually on a reduced dataset\n",
    "\n",
    "If we use the ted dataset, it's taking little more than an hour for a epoch\n",
    "\n",
    "Also, this model doesn't work from the training above, can you see why?\n",
    "\n",
    "<details>\n",
    "### Overfitting\n",
    "\n",
    "From GPT\n",
    "\n",
    "Overfitting in a seq2seq model using LSTMs can occur due to a number of factors. Here are the most likely ones:\n",
    "\n",
    "Insufficient Training Data: If the dataset is too small or doesn't adequately represent the variety of real-world data the model will encounter, the model can memorize the training data instead of learning generalizable patterns.\n",
    "\n",
    "Model Complexity: LSTM networks have a large number of parameters. If the architecture is too complex (too many layers or units), the model may overfit, especially with limited data.\n",
    "\n",
    "Lack of Regularization: If regularization techniques like dropout or L2 regularization (weight decay) are not applied, the model may overfit by relying too heavily on specific features of the training data.\n",
    "\n",
    "Training for Too Many Epochs: Training for too long without early stopping or monitoring the validation loss can lead to the model memorizing the training data.\n",
    "\n",
    "Noisy Data: If the training data contains a lot of noise (irrelevant or inconsistent information), the model may end up fitting that noise rather than learning the underlying patterns.\n",
    "\n",
    "Batch Size: A very small batch size can lead to noisy updates that could cause overfitting, while too large of a batch size might lead to poor generalization.\n",
    "\n",
    "Lack of Data Augmentation: For certain types of data (such as text), data augmentation techniques (like paraphrasing) can help increase the diversity of the training set and reduce overfitting.\n",
    "\n",
    "Do YOU have any fix to that?\n",
    "</details>"
   ],
   "id": "59c9d617a6595b98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### We save the model:",
   "id": "36d47a8a40fb8a1a"
  },
  {
   "cell_type": "code",
   "id": "5a3b6fd34ed1a2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "5a3b6fd34ed1a2b",
    "outputId": "242960ca-2e53-488e-951e-8969e77841ed"
   },
   "source": "model.save(\"seq2seq_model.h5\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "jEQAurao3Sdt",
   "metadata": {
    "id": "jEQAurao3Sdt"
   },
   "source": [
    "## Model Inference\n",
    "\n",
    "Below is the code to load to model as inference and translation on the user end"
   ]
  },
  {
   "cell_type": "code",
   "id": "64c17ce316e5adfd",
   "metadata": {
    "id": "64c17ce316e5adfd"
   },
   "source": [
    "from load_data import input_tokenizer, output_tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "model = load_model(\"seq2seq_model.h5\")\n",
    "\n",
    "# Extract Encoder Model\n",
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Define Decoder Model\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm[-1](decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Chat Function\n",
    "def decode_sequence(input_text):\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
    "    print(input_seq)\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=MAX_INPUT_LENGTH)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = output_tokenizer.word_index['sos']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = output_tokenizer.index_word.get(sampled_token_index, '')\n",
    "        decoded_sentence += sampled_word + \" \"\n",
    "        if sampled_word == 'eos' or len(decoded_sentence.split()) > MAX_OUTPUT_LENGTH:\n",
    "            stop_condition = True\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return decoded_sentence.strip()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caadc31af994afe9",
   "metadata": {
    "id": "caadc31af994afe9"
   },
   "source": [
    "\n",
    "# Test the Model\n",
    "print(decode_sequence(\"Ich bin ein Student\"))\n",
    "print(decode_sequence(\"Hallo, wie geht es dir?\"))\n",
    "print(decode_sequence(\"Guten Morgen\"))\n",
    "\n",
    "# Interactive Chatbot\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    response = decode_sequence(user_input)\n",
    "    print(\"Bot:\", response)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
