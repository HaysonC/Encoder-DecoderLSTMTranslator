{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sequence to Sequence Learning with Keras\n",
    "Author: Hayson Cheung [hayson.cheung@mail.utoronto.ca]\n",
    "\n",
    "In this notebook, we learn from the works of Ilya Sutskever, Oriol Vinyals, Quoc V. Le, Sequence to Sequence Learning with Neural Networks, NIPS 2014. We will implement a simple sequence to sequence model using LSTM in Keras. The model will be trained on a dataset of English sentences and their corresponding German sentences. The model will be able to translate English sentences to German sentences.\n",
    "\n",
    "We map sequences of English words to sequences of German words. The model is trained on a dataset of English sentences and their corresponding German sentences. The model will be able to translate English sentences to German sentences."
   ],
   "id": "5f5e165ae15d9f65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2e3d1a0d6d2f9cfb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-27T02:04:46.417141Z",
     "start_time": "2025-01-27T02:04:44.059117Z"
    }
   },
   "source": [
    "# sample.ipynb\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T02:04:46.423204Z",
     "start_time": "2025-01-27T02:04:46.420899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "\n",
    "# Latent dimension is the number of hidden units |h(t)| in the LSTM cell\n",
    "LATENT_DIM = 256  "
   ],
   "id": "fec8c0e7e94763e4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T02:04:58.179376Z",
     "start_time": "2025-01-27T02:04:46.480470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import load_data\n",
    "\n",
    "load_data.main(\"de-en.tmx\")\n",
    "\n",
    "from load_data import INPUT_VOCAB_SIZE, OUTPUT_VOCAB_SIZE, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH\n",
    "print(f\"Input vocab size: {INPUT_VOCAB_SIZE}\")\n",
    "print(f\"Output vocab size: {OUTPUT_VOCAB_SIZE}\")\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH}\")\n",
    "print(f\"Max output length: {MAX_OUTPUT_LENGTH}\")"
   ],
   "id": "bc6b4ce2521bad5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 98575\n",
      "Output vocab size: 45853\n",
      "Max input length: 663\n",
      "Max output length: 631\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ENCODER and DECODER\n",
    "\n",
    "In the two LSTM models, the encoder LSTM model will take the input sequence and return the encoder states. The decoder LSTM model will take the output sequence and the encoder states as input and return the output sequence. The encoder and decoder models are defined separately and then combined to form the final model."
   ],
   "id": "10014d442ffb5d01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T02:04:58.412254Z",
     "start_time": "2025-01-27T02:04:58.266716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Encoder\n",
    "encoder_input = Input(shape=(MAX_INPUT_LENGTH,))\n",
    "\n",
    "encoder_embedding = Embedding(INPUT_VOCAB_SIZE, LATENT_DIM)(encoder_input)\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define Decoder\n",
    "decoder_input = Input(shape=(MAX_OUTPUT_LENGTH,))\n",
    "decoder_embedding = Embedding(OUTPUT_VOCAB_SIZE, LATENT_DIM)(decoder_input)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "# Softmax means output is a probability distribution, and enhances the maximum probability output\n",
    "# dense layer is a regular densely-connected NN layer with softmax activation\n",
    "decoder_dense = Dense(OUTPUT_VOCAB_SIZE, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_outputs)"
   ],
   "id": "adcddf97abde4754",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T02:04:58.420799Z",
     "start_time": "2025-01-27T02:04:58.419153Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "254e40a8baeb2d78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T02:04:58.456157Z",
     "start_time": "2025-01-27T02:04:58.435099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "id": "b5680cca9a102c03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m663\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m631\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m663\u001B[0m, \u001B[38;5;34m256\u001B[0m)  │ \u001B[38;5;34m25,235,200\u001B[0m │ input_layer[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m] │\n",
       "│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m631\u001B[0m, \u001B[38;5;34m256\u001B[0m)  │ \u001B[38;5;34m11,738,368\u001B[0m │ input_layer_1[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│ (\u001B[38;5;33mEmbedding\u001B[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)         │ [(\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m),     │    \u001B[38;5;34m525,312\u001B[0m │ embedding[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "│                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m),      │            │                   │\n",
       "│                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)       │ [(\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m631\u001B[0m,      │    \u001B[38;5;34m525,312\u001B[0m │ embedding_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m… │\n",
       "│                     │ \u001B[38;5;34m256\u001B[0m), (\u001B[38;5;45mNone\u001B[0m,      │            │ lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m1\u001B[0m],       │\n",
       "│                     │ \u001B[38;5;34m256\u001B[0m), (\u001B[38;5;45mNone\u001B[0m,      │            │ lstm[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m2\u001B[0m]        │\n",
       "│                     │ \u001B[38;5;34m256\u001B[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m631\u001B[0m,       │ \u001B[38;5;34m11,784,221\u001B[0m │ lstm_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]      │\n",
       "│                     │ \u001B[38;5;34m45853\u001B[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">663</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">631</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">663</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">25,235,200</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">631</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,738,368</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">631</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">631</span>,       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,784,221</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">45853</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m49,808,413\u001B[0m (190.00 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,808,413</span> (190.00 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m49,808,413\u001B[0m (190.00 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,808,413</span> (190.00 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the Model\n",
    "This is where we train the model. We use the encoder input and decoder input to predict the decoder output. The model is trained on the dataset of English sentences and their corresponding German sentences.\n",
    "\n",
    "This takes a while to run. We can save the model and load it later."
   ],
   "id": "beb6acb2f8991bb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explaination of the data set\n",
    "encoder_input_train: Training data for the encoder (German sentences).\n",
    "decoder_input_train: Training data for the decoder (English sentences with <start> token).\n",
    "decoder_target_train: Target data for the decoder (English sentences).\n",
    "\n",
    "encoder_input_val: Validation data for the encoder (German sentences).\n",
    "decoder_input_val: Validation data for the decoder (English sentences with <start> token).\n",
    "decoder_target_val: Target data for the decoder (English sentences).\n",
    "\n"
   ],
   "id": "499b2636dcad0c08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-27T02:04:58.477924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Set Preparation\n",
    "from load_data import encoder_input_train, decoder_input_train, decoder_target_train, encoder_input_val, decoder_input_val, decoder_target_val\n",
    "\n",
    "model.fit(\n",
    "    [encoder_input_train, decoder_input_train],  # Inputs for encoder and decoder\n",
    "    decoder_target_train,  # Target data for decoder\n",
    "    batch_size=64,  # Adjust as needed\n",
    "    epochs=30,  # Adjust as needed\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),  # Validation data\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "8f75499163de2427",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m   1/3570\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m297:22:35\u001B[0m 300s/step - accuracy: 0.0000e+00 - loss: 10.7346"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As you can see, it would take forever to train the model. 3570s * 300 epochs = 1071000s = 297.5 hours = 12.4 days. We can save the model and load it later. At this rate, it takes a whole year to train the model. We can save the model and load it later.",
   "id": "6fd31bba1f272193"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a3b6fd34ed1a2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
